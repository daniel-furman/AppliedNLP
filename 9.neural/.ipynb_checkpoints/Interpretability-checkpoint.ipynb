{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df3a69eb",
   "metadata": {},
   "source": [
    "In this notebook, you'll use the method of integrated gradients to identify the tokens in the input that are most responsible for the predictions that a bigram CNN model is making.  Before running, install the captum library:\n",
    "\n",
    "```\n",
    "pip install captum\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06bcbbfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting captum\n",
      "  Downloading captum-0.4.1-py3-none-any.whl (1.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.4 MB 2.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: matplotlib in /Users/danielfurman/opt/anaconda3/envs/anlp/lib/python3.8/site-packages (from captum) (3.4.2)\n",
      "Requirement already satisfied: numpy in /Users/danielfurman/opt/anaconda3/envs/anlp/lib/python3.8/site-packages (from captum) (1.21.2)\n",
      "Requirement already satisfied: torch>=1.2 in /Users/danielfurman/opt/anaconda3/envs/anlp/lib/python3.8/site-packages (from captum) (1.9.0)\n",
      "Requirement already satisfied: typing_extensions in /Users/danielfurman/opt/anaconda3/envs/anlp/lib/python3.8/site-packages (from torch>=1.2->captum) (3.10.0.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /Users/danielfurman/opt/anaconda3/envs/anlp/lib/python3.8/site-packages (from matplotlib->captum) (8.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /Users/danielfurman/opt/anaconda3/envs/anlp/lib/python3.8/site-packages (from matplotlib->captum) (3.0.4)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/danielfurman/opt/anaconda3/envs/anlp/lib/python3.8/site-packages (from matplotlib->captum) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/danielfurman/opt/anaconda3/envs/anlp/lib/python3.8/site-packages (from matplotlib->captum) (0.10.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/danielfurman/opt/anaconda3/envs/anlp/lib/python3.8/site-packages (from matplotlib->captum) (2.8.2)\n",
      "Requirement already satisfied: six in /Users/danielfurman/opt/anaconda3/envs/anlp/lib/python3.8/site-packages (from cycler>=0.10->matplotlib->captum) (1.16.0)\n",
      "Installing collected packages: captum\n",
      "Successfully installed captum-0.4.1\n"
     ]
    }
   ],
   "source": [
    "!pip install captum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1d73712",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import nltk\n",
    "import random\n",
    "from captum.attr import LayerIntegratedGradients, visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f1226f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(x, y, batch_size=12):\n",
    "    batches_x=[]\n",
    "    batches_y=[]\n",
    "    for i in range(0, len(x), batch_size):\n",
    "        xbatch=x[i:i+batch_size]\n",
    "        ybatch=y[i:i+batch_size]\n",
    "        \n",
    "        maxlen=max([len(sent) for sent in xbatch])\n",
    "        \n",
    "        # pad sequence with 0's to maximum sequence length within that batch\n",
    "        for j in range(len(xbatch)):\n",
    "            xbatch[j].extend([0] * (maxlen-len(xbatch[j])))\n",
    "                        \n",
    "        batches_x.append(torch.LongTensor(xbatch))\n",
    "        batches_y.append(torch.LongTensor(ybatch))\n",
    "    \n",
    "    return batches_x, batches_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea1b58c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_INDEX = 0             # reserved for padding words\n",
    "UNKNOWN_INDEX = 1         # reserved for unknown words\n",
    "\n",
    "data_lens = []\n",
    "\n",
    "def read_embeddings(filename, vocab_size=100000):\n",
    "    \"\"\"\n",
    "  Utility function, loads in the `vocab_size` most common embeddings from `filename`\n",
    "  \n",
    "  Arguments:\n",
    "  - filename:     path to file\n",
    "                  automatically infers correct embedding dimension from filename\n",
    "  - vocab_size:   maximum number of embeddings to load\n",
    "\n",
    "  Returns \n",
    "  - embeddings:   torch.FloatTensor matrix of size (vocab_size x word_embedding_dim)\n",
    "  - vocab:        dictionary mapping word (str) to index (int) in embedding matrix\n",
    "  \"\"\"\n",
    "\n",
    "  # get the embedding size from the first embedding\n",
    "    with open(filename, encoding=\"utf-8\") as file:\n",
    "        word_embedding_dim = len(file.readline().split(\" \")) - 1\n",
    "\n",
    "    vocab = {\"[PAD]\":0, \"[UNK]\":1}\n",
    "\n",
    "    embeddings = np.zeros((vocab_size, word_embedding_dim))\n",
    "    with open(filename, encoding=\"utf-8\") as file:\n",
    "        for idx, line in enumerate(file):\n",
    "\n",
    "            if idx + 2 >= vocab_size:\n",
    "                break\n",
    "\n",
    "            cols = line.rstrip().split(\" \")\n",
    "            val = np.array(cols[1:])\n",
    "            word = cols[0]\n",
    "            embeddings[idx + 2] = val\n",
    "            vocab[word] = idx + 2\n",
    "  \n",
    "    return torch.FloatTensor(embeddings), vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ccb1eeb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings, vocab=read_embeddings(\"../data/glove.6B.100d.100K.txt\")\n",
    "rev_vocab={vocab[l]:l for l in vocab}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ba0cd896",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_labels(filename):\n",
    "    labels={}\n",
    "    with open(filename) as file:\n",
    "        for line in file:\n",
    "            cols = line.split(\"\\t\")\n",
    "            label = cols[0]\n",
    "            if label not in labels:\n",
    "                labels[label]=len(labels)\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f04ba65e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(filename, vocab, labels, max_data_points=1000):\n",
    "    \"\"\"\n",
    "    :param filename: the name of the file\n",
    "    :return: list of tuple ([word index list], label)\n",
    "    as input for the forward and backward function\n",
    "    \"\"\"    \n",
    "    data = []\n",
    "    data_labels = []\n",
    "    with open(filename) as file:\n",
    "        for line in file:\n",
    "            cols = line.split(\"\\t\")\n",
    "            label = cols[0]\n",
    "            text = cols[1]\n",
    "            w_int = []\n",
    "            for w in nltk.word_tokenize(text.lower()):\n",
    "                if w in vocab:\n",
    "                    w_int.append(vocab[w])\n",
    "                else:\n",
    "                    w_int.append(UNKNOWN_INDEX)\n",
    "                    \n",
    "            data.append((w_int))\n",
    "            data_labels.append(labels[label])\n",
    "            \n",
    "\n",
    "    # shuffle the data\n",
    "    tmp = list(zip(data, data_labels))\n",
    "    random.shuffle(tmp)\n",
    "    data, data_labels = zip(*tmp)\n",
    "    \n",
    "    if max_data_points is None:\n",
    "        return data, data_labels\n",
    "    \n",
    "    return data[:max_data_points], data_labels[:max_data_points]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fae773d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_data(text):\n",
    "    w_int = []\n",
    "    for w in nltk.word_tokenize(text.lower()):\n",
    "        if w in vocab:\n",
    "            w_int.append(vocab[w])\n",
    "        else:\n",
    "            w_int.append(UNKNOWN_INDEX)\n",
    "    return w_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6aaab605",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change this to the directory with your data (from the CheckData_TODO.ipynb exercise).  \n",
    "# The directory should contain train.tsv, dev.tsv and test.tsv\n",
    "directory=\"../data/lmrd\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9f4017aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels=read_labels(\"%s/train.tsv\" % directory)\n",
    "rev_labels={labels[l]:l for l in labels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "749953c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX, trainY=read_data(\"%s/train.tsv\" % directory, vocab, labels, max_data_points=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "704d9058",
   "metadata": {},
   "outputs": [],
   "source": [
    "devX, devY=read_data(\"%s/dev.tsv\" % directory, vocab, labels, max_data_points=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8dc129e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_trainX, batch_trainY=get_batches(trainX, trainY)\n",
    "batch_devX, batch_devY=get_batches(devX, devY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e2c58396",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNClassifier_bigram(nn.Module):\n",
    "\n",
    "    \"\"\"\n",
    "    CNN with a window size of 2 (i.e., 2grams) and 96 filters\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, pretrained_embeddings):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.num_filters=96\n",
    "        \n",
    "        self.num_labels = 2\n",
    "\n",
    "        _, embedding_dim=pretrained_embeddings.shape\n",
    "        \n",
    "        self.embeddings = nn.Embedding.from_pretrained(pretrained_embeddings, freeze=True)\n",
    "\n",
    "        # convolution over 2 words    \n",
    "        self.conv_2 = nn.Conv1d(embedding_dim, self.num_filters, 2, 1)\n",
    "        \n",
    "        self.fc = nn.Linear(self.num_filters, self.num_labels)\n",
    "\n",
    "    \n",
    "    def forward(self, input): \n",
    "        \n",
    "        # batch_size x max_seq_length x embeddings_size\n",
    "        x0 = self.embeddings(input)\n",
    "        \n",
    "        # batch_size x embeddings_size x max_seq_length\n",
    "        # (the input order expected by nn.Conv1d)\n",
    "        x0 = x0.permute(0, 2, 1)\n",
    "\n",
    "        # convolution\n",
    "        x2 = self.conv_2(x0)\n",
    "        # non-linearity\n",
    "        x2 = torch.tanh(x2)\n",
    "        # global max-pooling over the entire sequence\n",
    "        x2=torch.max(x2, 2)[0]\n",
    "\n",
    "        out = self.fc(x2)\n",
    "        \n",
    "        return out        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "962a5214",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, x, y):\n",
    "    model.eval()\n",
    "    corr = 0.\n",
    "    total = 0.\n",
    "    with torch.no_grad():\n",
    "        for x, y in zip(x, y):\n",
    "            y_preds=model.forward(x)\n",
    "            for idx, y_pred in enumerate(y_preds):\n",
    "                prediction=torch.argmax(y_pred)\n",
    "                if prediction == y[idx]:\n",
    "                    corr += 1.\n",
    "                total+=1                          \n",
    "    return corr/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "95c55241",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, x):\n",
    "    model.eval()\n",
    "    preds=[]\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_x in x:\n",
    "            y_preds=model.forward(batch_x).numpy()\n",
    "            for y_pred in y_preds:\n",
    "                prediction=np.argmax(y_pred)\n",
    "                preds.append(prediction)\n",
    "                \n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b57163a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, model_filename, train_batches_x, train_batches_y, dev_batches_x, dev_batches_y):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "    losses = []\n",
    "    cross_entropy=nn.CrossEntropyLoss()\n",
    "\n",
    "    best_dev_acc=0.\n",
    "    \n",
    "    for epoch in range(5):\n",
    "        model.train()\n",
    "\n",
    "        for x, y in zip(train_batches_x, train_batches_y):\n",
    "            y_pred=model.forward(x)\n",
    "            loss = cross_entropy(y_pred.view(-1, 2), y.view(-1))\n",
    "            losses.append(loss)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        dev_accuracy=evaluate(model, dev_batches_x, dev_batches_y)\n",
    "        \n",
    "        # we're going to save the model that performs the best on *dev* data\n",
    "        if dev_accuracy > best_dev_acc:\n",
    "            torch.save(model.state_dict(), model_filename)\n",
    "            print(\"%.3f is better than %.3f, saving model ...\" % (dev_accuracy, best_dev_acc))\n",
    "            best_dev_acc = dev_accuracy\n",
    "        if epoch % 1 == 0:\n",
    "            print(\"Epoch %s, dev accuracy: %.3f\" % (epoch, dev_accuracy))\n",
    "            \n",
    "    model.load_state_dict(torch.load(model_filename))            \n",
    "    print(\"\\nBest Performing Model achieves dev accuracy of : %.3f\" % (best_dev_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d36b85",
   "metadata": {},
   "source": [
    "First, let's train our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1e167f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model = CNNClassifier_bigram(pretrained_embeddings=embeddings)\n",
    "train(cnn_model, \"cnn.bigram.model\", batch_trainX, batch_trainY, batch_devX, batch_devY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "744af199",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpret(x, y, model, vocab, rev_labels, rev_vocab):\n",
    "    \n",
    "    ''' https://captum.ai/tutorials/IMDB_TorchText_Interpret '''\n",
    "    \n",
    "    model.eval()\n",
    "    _, maxlen=x.shape\n",
    "    \n",
    "    # baseline is uninformative sequence of padding tokens\n",
    "    baseline=torch.LongTensor([[PAD_INDEX]*maxlen])\n",
    "    y_preds=model.forward(x)\n",
    "    \n",
    "    y_preds=torch.nn.functional.softmax(y_preds, dim=1)\n",
    "    y_preds=y_preds.detach().numpy()\n",
    "    preds=[]\n",
    "    for y_pred in y_preds:\n",
    "        prediction=np.argmax(y_pred)\n",
    "        preds.append(prediction)\n",
    "    \n",
    "    # we'll get our attributions with respect to target class #1\n",
    "    target_class=1\n",
    "    \n",
    "    ig = LayerIntegratedGradients(cnn_model, cnn_model.embeddings)\n",
    "    attributions, delta = ig.attribute(x, baseline, target=target_class, return_convergence_delta=True)\n",
    "    attributions = attributions.sum(dim=2).squeeze(0)\n",
    "    attributions = attributions / torch.norm(attributions)\n",
    "    attributions = attributions.numpy()\n",
    "    \n",
    "    orig=[]\n",
    "    \n",
    "    for idx, sent in enumerate(x):\n",
    "        orig.append([])\n",
    "        for tok in sent:\n",
    "            tok=int(tok.numpy())\n",
    "            orig[idx].append(rev_vocab[tok])\n",
    "    \n",
    "    y=y.numpy()\n",
    "\n",
    "    records=[]\n",
    "    for idx, pred in enumerate(preds):\n",
    "        records.append(visualization.VisualizationDataRecord(\n",
    "                                    attributions[idx],\n",
    "                                    y_preds[idx][0],\n",
    "                                    rev_labels[preds[idx]],\n",
    "                                    rev_labels[y[idx]],\n",
    "                                    rev_labels[target_class],\n",
    "                                    attributions[idx].sum(),\n",
    "                                    orig[idx],\n",
    "                                    delta))\n",
    "    visualization.visualize_text(records, legend=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7790bdb9",
   "metadata": {},
   "source": [
    "**Q1**. Create a smaller set of toy examples here to interpret this method on relatively short texts.  How does this accord with your understanding of what a bigram CNN should be paying attention to?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960379b8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x=[\"The writing was amazing and Daniel Day-Lewis was terrific!\", \"Terrible!\", \"This movie is not bad\", \"Exactly what I was looking for.\", \"Outrageously good.\"]\n",
    "y=[\"pos\", \"neg\", \"pos\", \"pos\", \"pos\"]\n",
    "batch_x, batch_y=get_batches([transform_data(xs) for xs in x], [labels[ys] for ys in y])\n",
    "interpret(batch_x[0], batch_y[0], cnn_model, vocab, rev_labels, rev_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd48822b",
   "metadata": {},
   "source": [
    "**Q2**. Read in a batch of your development data and examine the terms that are identified as being most important in the input.  Are they what you would expect?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d7e5376",
   "metadata": {},
   "outputs": [],
   "source": [
    "interpret(batch_devX[0], batch_devY[0], cnn_model, vocab, rev_labels, rev_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97056e33",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
