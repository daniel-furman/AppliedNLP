{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The log-odds ratio with an informative (and uninformative) Dirichlet prior (described in [Monroe et al. 2009, Fighting Words](http://languagelog.ldc.upenn.edu/myl/Monroe.pdf)) is a common method for finding distinctive terms in two datasets (see [Jurafsky et al. 2014](https://firstmonday.org/ojs/index.php/fm/article/view/4944/3863) for an example article that uses it to make an empirical argument). This method for finding distinguishing words combines a number of desirable properties:\n",
    "\n",
    "* it specifies an intuitive metric (the log-odds) for the ratio of two probabilities\n",
    "* it incorporates prior information in the form of pseudocounts, which can either act as a smoothing factor (in the uninformative case) or incorporate real information about the expected frequency of words overall.\n",
    "* it accounts for variability of a frequency estimate by essentially converting the log-odds to a z-score.\n",
    "\n",
    "In this homework you will implement both of these ratios and compare the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, operator, math, nltk\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_and_tokenize(filename):\n",
    "    \n",
    "    with open(filename, encoding=\"utf-8\") as file:\n",
    "        tokens=[]\n",
    "        # lowercase\n",
    "        for line in file:\n",
    "            data=line.rstrip().lower()\n",
    "            # This dataset is already tokenized, so we can split on whitespace\n",
    "            tokens.extend(data.split(\" \"))\n",
    "        return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data we'll use in this case comes from a sample of 1000 positive and 1000 negative movie reviews from the [Large Movie Review Dataset](https://ai.stanford.edu/~amaas/data/sentiment/).  The version of the data used in this homework has already been tokenized for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_tokens=read_and_tokenize(\"../data/negative.reviews.txt\")\n",
    "positive_tokens=read_and_tokenize(\"../data/positive.reviews.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1.  Implement the log-odds ratio with an uninformative Dirichlet prior.  This value, $\\hat\\zeta_w^{(i-j)}$ for word $w$ reflecting the difference in usage between corpus $i$ and corpus $j$, is given by the following equation:\n",
    "\n",
    "$$\n",
    "\\hat\\zeta_w^{(i-j)}= {\\hat{d}_w^{(i-j)} \\over \\sqrt{\\sigma^2\\left(\\hat{d}_w^{(i-j)}\\right)}}\n",
    "$$\n",
    "\n",
    "Where: \n",
    "\n",
    "$$\n",
    "\\hat{d}_w^{(i-j)} = \\log \\left({y_w^i + \\alpha_w} \\over {n^i + \\alpha_0 - y_w^i - \\alpha_w}) \\right) -  \\log \\left({y_w^j + \\alpha_w} \\over {n^j + \\alpha_0 - y_w^j - \\alpha_w}) \\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\sigma^2\\left(\\hat{d}_w^{(i-j)}\\right) \\approx {1 \\over {y_w^i + \\alpha_w}} + {1 \\over {y_w^j + \\alpha_w} }\n",
    "$$\n",
    "\n",
    "And:\n",
    "\n",
    "* $y_w^i = $ count of word $w$ in corpus $i$ (likewise for $j$)\n",
    "* $\\alpha_w$ = 0.01\n",
    "* $V$ = size of vocabulary (number of distinct word types)\n",
    "* $\\alpha_0 = V * \\alpha_w$\n",
    "* $n^i = $ number of words in corpus $i$ (likewise for $j$)\n",
    "\n",
    "Here the two corpora are the positive movie reviews (e.g., $i$ = positive) and the negative movie reviews (e.g., $j$ = negative). Using this metric, print out the 25 words most strongly aligned with the positive corpus, and 25 words most strongly aligned with the negative corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logodds_with_uninformative_prior(one_tokens, two_tokens, display=25):\n",
    "    \n",
    "    def get_counter_from_list(tokens):\n",
    "        counter=Counter()\n",
    "        for token in tokens:\n",
    "            counter[token]+=1\n",
    "        return counter\n",
    "\n",
    "\n",
    "    oneCounter=get_counter_from_list(one_tokens)\n",
    "    twoCounter=get_counter_from_list(two_tokens)\n",
    "    \n",
    "    vocab=dict(oneCounter) \n",
    "    vocab.update(dict(twoCounter))\n",
    "    oneSum=sum(oneCounter.values())\n",
    "    twoSum=sum(twoCounter.values())\n",
    "\n",
    "    ranks={}\n",
    "    alpha=0.01\n",
    "    alphaV=len(vocab)*alpha\n",
    "        \n",
    "    for word in vocab:\n",
    "        \n",
    "        log_odds_ratio=math.log( (oneCounter[word] + alpha) / (oneSum+alphaV-oneCounter[word]-alpha) ) - math.log( (twoCounter[word] + alpha) / (twoSum+alphaV-twoCounter[word]-alpha) )\n",
    "        variance=1./(oneCounter[word] + alpha) + 1./(twoCounter[word] + alpha)\n",
    "        \n",
    "        ranks[word]=log_odds_ratio/math.sqrt(variance)\n",
    "\n",
    "    sorted_x = sorted(ranks.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    \n",
    "    print(\"Most positive:\")\n",
    "    for k,v in sorted_x[:display]:\n",
    "        print(\"%.3f\\t%s\" % (v,k))\n",
    "    \n",
    "    print(\"\\nMost negative:\")\n",
    "    for k,v in reversed(sorted_x[-display:]):\n",
    "        print(\"%.3f\\t%s\" % (v,k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most positive:\n",
      "9.608\tgreat\n",
      "8.445\this\n",
      "8.221\tbest\n",
      "8.068\tas\n",
      "8.008\tand\n",
      "7.466\tlove\n",
      "7.232\twar\n",
      "7.143\texcellent\n",
      "6.697\twonderful\n",
      "6.559\tis\n",
      "6.389\ther\n",
      "6.052\tperformance\n",
      "5.937\t,\n",
      "5.769\tof\n",
      "5.722\tlife\n",
      "5.707\thighly\n",
      "5.664\tworld\n",
      "5.547\tperfect\n",
      "5.490\tin\n",
      "5.466\talways\n",
      "5.380\tperformances\n",
      "5.356\tbeautiful\n",
      "5.198\tmost\n",
      "5.148\ttony\n",
      "5.092\tloved\n",
      "\n",
      "Most negative:\n",
      "-15.874\tbad\n",
      "-15.035\t?\n",
      "-11.949\tn't\n",
      "-10.960\tmovie\n",
      "-9.929\tworst\n",
      "-9.448\ti\n",
      "-9.122\tjust\n",
      "-8.676\t...\n",
      "-8.618\twas\n",
      "-7.999\tno\n",
      "-7.521\tdo\n",
      "-7.512\tawful\n",
      "-7.446\tterrible\n",
      "-7.373\tthey\n",
      "-7.053\thorrible\n",
      "-7.020\twhy\n",
      "-6.935\tthis\n",
      "-6.931\tpoor\n",
      "-6.709\tboring\n",
      "-6.685\tany\n",
      "-6.674\twaste\n",
      "-6.661\tscript\n",
      "-6.601\tworse\n",
      "-6.552\thave\n",
      "-6.475\tstupid\n"
     ]
    }
   ],
   "source": [
    "logodds_with_uninformative_prior(positive_tokens, negative_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2: As you increase the constant $\\alpha_w$ in the equations above, what happens to $\\hat\\zeta_w^{(i-j)}$, $\\hat{d}_w^{(i-j)}$ and $\\sigma^2\\left(\\hat{d}_w^{(i-j)}\\right)$ (i.e., do they get bigger or smaller)?  Answer this by plugging the following values in your implementation of these two quantities, and varying $\\alpha_w$ (and, consequently, $\\alpha_0$).\n",
    "\n",
    "* $y_w^i=34$\n",
    "* $y_w^j=17$\n",
    "* $n^i=1000$\n",
    "* $n^j=1000$\n",
    "* $V=500$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01:\t0.710\t0.088\t2.392\n",
      "0.1:\t0.707\t0.088\t2.385\n",
      "1:\t0.677\t0.084\t2.332\n",
      "10:\t0.491\t0.060\t2.009\n",
      "100:\t0.136\t0.016\t1.075\n",
      "1000:\t0.017\t0.002\t0.376\n"
     ]
    }
   ],
   "source": [
    "y_i=34\n",
    "y_j=17\n",
    "oneSum=1000\n",
    "twoSum=1000\n",
    "V=500\n",
    "for alpha in [0.01, 0.1, 1, 10, 100, 1000]:\n",
    "    alphaV=V*alpha\n",
    "    log_odds_ratio=math.log( (y_i + alpha) / (oneSum+alphaV-y_i-alpha) ) - math.log( (y_j + alpha) / (twoSum+alphaV-y_j-alpha) )\n",
    "    variance=1./(y_i + alpha) + 1./(y_j + alpha)\n",
    "\n",
    "    print(\"%s:\\t%.3f\\t%.3f\\t%.3f\" % (alpha, log_odds_ratio, variance, log_odds_ratio/math.sqrt(variance)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's make that prior informative by including information about the overall frequency of a given word in a background corpus (i.e., a corpus that represents general word usage, without regard for labeled subcorpora).  To do so, there are only two small changes to make:\n",
    "\n",
    "* We need to gather a background corpus $b$ and calculate $\\hat\\pi_w$, the relative frequency of word $w$ in $b$ (i.e., the number of times $w$ occurs in $b$ divided by the number of words in $b$).\n",
    "\n",
    "* In the uninformative prior above, $\\alpha_w$ was a constant (0.01) and $\\alpha_0 = V * \\alpha_w$.  Let us now set $\\alpha_0 = 1000$ and $\\alpha_w = \\hat\\pi_w * \\alpha_0$.  This reflects a pseudocount capturing the fractional number of times we would expect to see word $w$ in a sample of 1000 words.\n",
    "\n",
    "This allows us to specify that a common word like \"the\" (which has a relative frequency of $\\approx 0.04$) would have $\\alpha_w = 40$, while an infrequent word like \"beneficiaries\" (relative frequency $\\approx 0.00002$) would have $\\alpha_w = 0.02$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3. Implement a log-odds ratio with informative prior, using a larger background corpus of 5M tokens drawn from the same dataset (given to you as `priors` below, which contains the relative frequencies of words calculated from that corpus) and set $\\alpha_0 = 1000$. Using this metric, print out again the 25 words most strongly aligned with the positive corpus, and 25 words most strongly aligned with the negative corpus.  Is there a meaningful difference?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_priors(filename):\n",
    "    counts=Counter()\n",
    "    freqs={}\n",
    "    tokens=read_and_tokenize(filename)\n",
    "    total=len(tokens)\n",
    "\n",
    "    for token in tokens:\n",
    "        counts[token]+=1\n",
    "\n",
    "    for word in counts:\n",
    "        freqs[word]=counts[word]/total\n",
    "\n",
    "    return freqs\n",
    "    \n",
    "priors=read_priors(\"../data/sentiment.background.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logodds_with_informative_prior(one_tokens, two_tokens, priors, display=25):\n",
    "    \n",
    "    def get_counter_from_list(tokens):\n",
    "        counter=Counter()\n",
    "        for token in tokens:\n",
    "            counter[token]+=1\n",
    "        return counter\n",
    "    \n",
    "    a0=1000\n",
    "        \n",
    "    oneCounter=get_counter_from_list(one_tokens)\n",
    "    twoCounter=get_counter_from_list(two_tokens)\n",
    "    \n",
    "    vocab=dict(oneCounter) \n",
    "    vocab.update(dict(twoCounter))\n",
    "    oneSum=sum(oneCounter.values())\n",
    "    twoSum=sum(twoCounter.values())\n",
    "\n",
    "    ranks={}\n",
    "\n",
    "    for word in vocab:\n",
    "\n",
    "        prior=priors[word]*a0\n",
    "\n",
    "        log_odds_ratio=math.log( (oneCounter[word] + prior) / (oneSum+a0-oneCounter[word]-prior) ) - math.log( (twoCounter[word] + prior) / (twoSum+a0-twoCounter[word]-prior) )\n",
    "        variance=1./(oneCounter[word] + prior) + 1./(twoCounter[word] + prior)\n",
    "        \n",
    "        ranks[word]=log_odds_ratio/math.sqrt(variance)\n",
    "\n",
    "    sorted_x = sorted(ranks.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    \n",
    "    print(\"Most positive:\")\n",
    "    for k,v in sorted_x[:display]:\n",
    "        print(\"%.3f\\t%s\" % (v,k))\n",
    "    \n",
    "    print(\"\\nMost negative:\")\n",
    "    for k,v in reversed(sorted_x[-display:]):\n",
    "        print(\"%.3f\\t%s\" % (v,k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most positive:\n",
      "9.591\tgreat\n",
      "8.428\this\n",
      "8.207\tbest\n",
      "8.052\tas\n",
      "7.990\tand\n",
      "7.454\tlove\n",
      "7.226\twar\n",
      "7.136\texcellent\n",
      "6.693\twonderful\n",
      "6.544\tis\n",
      "6.377\ther\n",
      "6.043\tperformance\n",
      "5.922\t,\n",
      "5.755\tof\n",
      "5.712\tlife\n",
      "5.704\thighly\n",
      "5.655\tworld\n",
      "5.540\tperfect\n",
      "5.477\tin\n",
      "5.456\talways\n",
      "5.371\tperformances\n",
      "5.346\tbeautiful\n",
      "5.189\tmost\n",
      "5.152\ttony\n",
      "5.085\tloved\n",
      "\n",
      "Most negative:\n",
      "-15.858\tbad\n",
      "-15.012\t?\n",
      "-11.930\tn't\n",
      "-10.942\tmovie\n",
      "-9.958\tworst\n",
      "-9.434\ti\n",
      "-9.107\tjust\n",
      "-8.661\t...\n",
      "-8.604\twas\n",
      "-7.986\tno\n",
      "-7.513\tawful\n",
      "-7.509\tdo\n",
      "-7.450\tterrible\n",
      "-7.361\tthey\n",
      "-7.055\thorrible\n",
      "-7.008\twhy\n",
      "-6.925\tthis\n",
      "-6.923\tpoor\n",
      "-6.719\twaste\n",
      "-6.702\tboring\n",
      "-6.674\tany\n",
      "-6.652\tscript\n",
      "-6.597\tworse\n",
      "-6.541\thave\n",
      "-6.468\tstupid\n"
     ]
    }
   ],
   "source": [
    "logodds_with_informative_prior(positive_tokens, negative_tokens, priors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
