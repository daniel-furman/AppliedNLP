{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook explores identifying multiword expressions using the part-of-speech filtering technique of Justeson and Katz (1995), \"[Technical terminology: some linguistic properties and an algorithm for identification in text](https://brenocon.com/JustesonKatz1995.pdf)\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy, re\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('parser', <spacy.pipeline.dep_parser.DependencyParser at 0x7ff0382d6ee0>)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp = spacy.load('en_core_web_sm', disable=['ner,parser'])\n",
    "nlp.remove_pipe('ner')\n",
    "nlp.remove_pipe('parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTokens(filename):\n",
    "    \n",
    "    \"\"\" Read the first 1000 lines of an input file \"\"\"\n",
    "    tokens=[]\n",
    "    with open(filename) as file:\n",
    "        for idx,line in enumerate(file):\n",
    "            tokens.extend(nlp(line))\n",
    "            if idx > 1000:\n",
    "                break\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13815\n"
     ]
    }
   ],
   "source": [
    "tokens=getTokens(\"../data/odyssey.txt\")\n",
    "print(len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "words=[x.text for x in tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's simplify the POS tags to make the regex easier to understand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "adjectives=set([\"JJ\", \"JJR\", \"JJS\"])\n",
    "nouns=set([\"NN\", \"NNS\", \"NNP\", \"NNPS\"])\n",
    "\n",
    "taglist=[]\n",
    "for x in tokens:\n",
    "    if x.tag_ in adjectives:\n",
    "        taglist.append(\"ADJ\")\n",
    "    elif x.tag_ in nouns:\n",
    "        taglist.append(\"NOUN\")\n",
    "    elif x.tag == \"IN\":\n",
    "        taglist.append(\"PREP\")\n",
    "    else:\n",
    "        taglist.append(\"O\")\n",
    "                \n",
    "tags=' '.join(taglist)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getChar2TokenMap(tags):\n",
    "    \n",
    "    \"\"\"  We'll search over the postag sequence, so we need to get the token ID for any\n",
    "    character to be able to match the word token. \"\"\"\n",
    "    \n",
    "    ws=re.compile(\" \")\n",
    "    char2token={}\n",
    "\n",
    "    lastStart=0\n",
    "    for idx, m in enumerate(ws.finditer(tags)):\n",
    "        char2token[lastStart]=idx\n",
    "        lastStart=m.start()+1\n",
    "        \n",
    "    return char2token\n",
    "\n",
    "def getToken(tokenId, char2token):\n",
    "    \n",
    "    \"\"\" Find the token ID for given character in the POS sequence \"\"\"\n",
    "    while(tokenId > 0):\n",
    "        if tokenId in char2token:\n",
    "            return char2token[tokenId]\n",
    "        tokenId-=1\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "char2token=getChar2TokenMap(tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's find all sequences of POS tags that match the Justeson and Katz pattern of `(((ADJ|NOUN) )+|((ADJ|NOUN) )*(NOUN PREP )((ADJ|NOUN) )*)NOUN`\n",
    "\n",
    "\"In words, a candidate term is a multi-word noun phrase; and it either is a string of nouns and/or adjectives, ending in a noun, or it consists of two such strings, separated by a single preposition.\" (JK 17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "own house 5\n",
      "other hand 4\n",
      "marriage gifts 3\n",
      "barley meal 3\n",
      "own people 2\n",
      "other gods 2\n",
      "girt island 2\n",
      "poor man 2\n",
      "immortal gods 2\n",
      "dear father 2\n",
      "old woman 2\n",
      "funeral rites 2\n",
      "due pomp 2\n",
      "sad tale 2\n",
      "old friend 2\n",
      "outer court 2\n",
      "good old woman 2\n",
      "other women 2\n",
      "public moment 2\n",
      "excellent person 2\n",
      "old man 2\n",
      "leathern bags 2\n",
      "bad end 2\n",
      "false Aegisthus 2\n",
      "MINERVA â€™S VISIT 1\n",
      "ingenious hero 1\n",
      "famous town 1\n",
      "Many cities 1\n",
      "own life 1\n",
      "own sheer folly 1\n",
      "god Hyperion 1\n",
      "whatsoever source 1\n",
      "goddess Calypso 1\n",
      "large cave 1\n",
      "other East.1 1\n",
      "Olympian Jove 1\n",
      "own folly 1\n",
      "good will 1\n",
      "lonely sea 1\n",
      "magician Atlas 1\n",
      "great columns 1\n",
      "poor unhappy Ulysses 1\n",
      "own chimneys 1\n",
      "capable man 1\n",
      "Polyphemus king 1\n",
      "nymph Thoosa 1\n",
      "king Phorcys 1\n",
      "Ogygian island 1\n",
      "son Telemachus 1\n",
      "golden sandals 1\n",
      "redoubtable bronze 1\n",
      "shod spear 1\n",
      "topmost summits 1\n",
      "bronze spear 1\n",
      "lordly suitors 1\n",
      "wet sponges 1\n",
      "great quantities 1\n",
      "brave father 1\n",
      "right hand 1\n",
      "many other spears 1\n",
      "unhappy father 1\n",
      "maid servant 1\n",
      "beautiful golden ewer 1\n",
      "silver basin 1\n",
      "clean table 1\n",
      "upper servant 1\n",
      "many good things 1\n",
      "Forthwith men servants 1\n",
      "good things 1\n",
      "longer legs 1\n",
      "longer purse 1\n",
      "ill fate 1\n",
      "old days 1\n",
      "many visitors 1\n",
      "open country 1\n",
      "harbour Rheithron5 1\n",
      "wooded mountain 1\n",
      "old Laertes 1\n",
      "mid ocean 1\n",
      "such resource 1\n",
      "close friends 1\n",
      "wise child 1\n",
      "own father 1\n",
      "own estates 1\n",
      "fine son 1\n",
      "whole house 1\n",
      "respectable person 1\n",
      "mortal man 1\n",
      "woodland island 1\n",
      "principal men 1\n",
      "Ulysses home 1\n",
      "rascally suitors 1\n",
      "own threshold 1\n",
      "sorry wedding 1\n",
      "Achaean heroes 1\n",
      "morrow morning 1\n",
      "own place 1\n",
      "best ship 1\n",
      "fair means 1\n",
      "murderer Aegisthus 1\n"
     ]
    }
   ],
   "source": [
    "p = re.compile(\"(((ADJ|NOUN) )+|((ADJ|NOUN) )*(NOUN PREP )((ADJ|NOUN) )*)NOUN\")\n",
    "\n",
    "mweCount=Counter()\n",
    "\n",
    "for m in p.finditer(tags):\n",
    "    startToken=getToken(m.start(),char2token)\n",
    "    endToken=getToken(m.end(),char2token)\n",
    "    mwe=' '.join(words[startToken:endToken+1])\n",
    "    mweCount[mwe]+=1\n",
    "\n",
    "for k,v in mweCount.most_common(100):\n",
    "    print(k,v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll define our MWE dictionary to be the 1000 most frequent sequences matched."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_mwe=[k for (k,v) in mweCount.most_common(1000)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's transform each MWE into a single token (e.g., replace `New York City` with `New_York_City`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replaceMWE(text, mweList):\n",
    "    \n",
    "    \"\"\" Replace all instances of MWEs in text with single token \n",
    "    \n",
    "    MWEs are ranked from longest to shortest so that longest replacements are made first (e.g.,\n",
    "    \"New York City\" is matched first before \"New York\")\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    sorted_by_length = sorted(mweList, key=len, reverse=True)\n",
    "    for mwe in sorted_by_length:\n",
    "        text=re.sub(re.escape(mwe), re.sub(\" \", \"_\", mwe), text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "processedText=replaceMWE(\"The poor man, is one who is a dear father\", my_mwe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The poor_man, is one who is a dear_father\n"
     ]
    }
   ],
   "source": [
    "print(processedText)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1. Plug in your own data into `getTokens` above and identify the MWE it contains.  How do you think MWE would perform for your classification task?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d4222c62deee789526b2e6b735eb2f20b12feb76cd92f9c10ded5d55a4a9b08d"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('anlp': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
