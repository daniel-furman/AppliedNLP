{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thie notebook explores logistic regression and feedforward neural networks for binary text classification for your text classification problem, using the pytorch library. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn import preprocessing\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import random\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(filename, max_data_points=None):\n",
    "    X=[]\n",
    "    Y=[]\n",
    "    with open(filename, encoding=\"utf-8\") as file:\n",
    "        for idx,line in enumerate(file):\n",
    "            cols=line.rstrip().split(\"\\t\")\n",
    "            label=cols[0]\n",
    "            text=cols[1]\n",
    "            X.append(text)\n",
    "            Y.append(label)\n",
    "\n",
    "    # shuffle the data\n",
    "    tmp = list(zip(X, Y))\n",
    "    random.shuffle(tmp)\n",
    "    X, Y = zip(*tmp)\n",
    "    \n",
    "    if max_data_points == None:\n",
    "        return X, Y\n",
    "    \n",
    "    return X[:max_data_points], Y[:max_data_points]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change this to the directory with your data (from the CheckData_TODO.ipynb exercise).  \n",
    "# The directory should contain train.tsv, dev.tsv and test.tsv\n",
    "directory=\"../data/lmrd\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll limit the training and dev data to 10,000 data points for this exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX, trainY=read_data(\"%s/train.tsv\" % directory, max_data_points=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "devX, devY=read_data(\"%s/dev.tsv\" % directory, max_data_points=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll represent the data using simple binary indicators of the most frequent 1000 words in the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(max_features=1000, analyzer=str.split, lowercase=True, strip_accents=None, binary=True)\n",
    "X_train = vectorizer.fit_transform(trainX)\n",
    "X_dev = vectorizer.transform(devX)\n",
    "\n",
    "_,vocabSize=X_train.shape\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(trainY)\n",
    "\n",
    "Y_train=le.transform(trainY)\n",
    "Y_dev=le.transform(devY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(x, y, batch_size=12):\n",
    "    batches_x=[]\n",
    "    batches_y=[]\n",
    "    for i in range(0, len(x), batch_size):\n",
    "        batches_x.append(x[i:i+batch_size])\n",
    "        batches_y.append(y[i:i+batch_size])\n",
    "    \n",
    "    return batches_x, batches_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batches_x, train_batches_y=get_batches(torch.from_numpy(X_train.todense()).float(), torch.LongTensor(Y_train))\n",
    "dev_batches_x, dev_batches_y=get_batches(torch.from_numpy(X_dev.todense()).float(), torch.LongTensor(Y_dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, x, y):\n",
    "\n",
    "    model.eval()\n",
    "    corr = 0.\n",
    "    total = 0.\n",
    "    with torch.no_grad():\n",
    "        for x, y in zip(x, y):\n",
    "            y_preds=model.forward(x)\n",
    "            for idx, y_pred in enumerate(y_preds):\n",
    "                prediction=torch.argmax(y_pred)\n",
    "                if prediction == y[idx]:\n",
    "                    corr += 1.\n",
    "                total+=1                          \n",
    "    return corr/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegressionClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        # torch.nn.Linear transforms an input of size input_dim (e.g., 1000 above) to an output of size output_dim (e.g., 2 classes for positive/negative)\n",
    "        self.linear1 = torch.nn.Linear(input_dim, output_dim) \n",
    "    \n",
    "    def forward(self, input): \n",
    "        x1 = self.linear1(input)\n",
    "\n",
    "        return x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFNN_1_Hidden_Layer(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, output_dim, hidden_dim=100):\n",
    "        super().__init__()\n",
    "        \n",
    "        hidden_dim=100\n",
    "        # the first layer transforms an input of size input_dim (e.g., 1000 above) to an output of size hidden_dim (e.g., 100)\n",
    "        self.linear1 = torch.nn.Linear(input_dim, hidden_dim)\n",
    "\n",
    "        # the second layer transforms an input of size hidden_dim (e.g., 100) to an output of size output_dim (e.g., 2 classes for positive/negative)       \n",
    "        self.linear2 = torch.nn.Linear(hidden_dim, output_dim)\n",
    "    \n",
    "    def forward(self, input): \n",
    "        # pass the input through the first layer\n",
    "        layer1_output = self.linear1(input)\n",
    "        \n",
    "        # pass that output through a non-linearity (here, tanh)\n",
    "        layer1_output = torch.tanh(layer1_output)\n",
    "        \n",
    "        # and then pass the output from that first layer as input to the second layer\n",
    "        layer2_output = self.linear2(layer1_output)\n",
    "\n",
    "        return layer2_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, model_filename, train_batches_x, train_batches_y, dev_batches_x, dev_batches_y):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "    losses = []\n",
    "    cross_entropy=nn.CrossEntropyLoss()\n",
    "\n",
    "    best_dev_acc=0.\n",
    "    \n",
    "    # we'll only train for 5 epochs for this exercise, but in practice you'd want to train for more\n",
    "    # (in general until you stop seeing improvements in accuracy on your *development* data)\n",
    "    \n",
    "    for epoch in range(5):\n",
    "        model.train()\n",
    "\n",
    "        for x, y in zip(train_batches_x, train_batches_y):\n",
    "            y_pred=model.forward(x)\n",
    "            loss = cross_entropy(y_pred.view(-1, 2), y.view(-1))\n",
    "            losses.append(loss)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        dev_accuracy=evaluate(model, dev_batches_x, dev_batches_y)\n",
    "        \n",
    "        # we're going to save the model that performs the best on *dev* data\n",
    "        if dev_accuracy > best_dev_acc:\n",
    "            torch.save(model.state_dict(), model_filename)\n",
    "            print(\"%.3f is better than %.3f, saving model ...\" % (dev_accuracy, best_dev_acc))\n",
    "            best_dev_acc = dev_accuracy\n",
    "        if epoch % 1 == 0:\n",
    "            print(\"Epoch %s, dev accuracy: %.3f\" % (epoch, dev_accuracy))\n",
    "            \n",
    "    model.load_state_dict(torch.load(model_filename))            \n",
    "    print(\"\\nBest Performing Model achieves dev accuracy of : %.3f\" % (best_dev_acc))\n",
    "    return best_dev_acc\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg=LogisticRegressionClassifier(1000, 2)\n",
    "dev_accuracy=train(logreg, \"logreg.model\", train_batches_x, train_batches_y, dev_batches_x, dev_batches_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ffnn1=FFNN_1_Hidden_Layer(1000, 2, hidden_dim=100)\n",
    "dev_accuracy=train(ffnn1, \"ffnn1.model\", train_batches_x, train_batches_y, dev_batches_x, dev_batches_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural networks converge to different solutions as a function of their *initialization* (the random choice of the initial values for parameters).  Let's train the `FFNN_1_Hidden_Layer` model 10 times and then plot the distribution of dev accuracies using [pandas.DataFrame.plot.density](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.plot.density.html). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_accuracies=[]\n",
    "\n",
    "for i in range(10):\n",
    "    ffnn1=FFNN_1_Hidden_Layer(1000, 2, hidden_dim=100)\n",
    "    dev_accuracy=train(ffnn1, \"ffnn1.model\", train_batches_x, train_batches_y, dev_batches_x, dev_batches_y)\n",
    "    dev_accuracies.append(dev_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame(dev_accuracies)\n",
    "ax = df.plot.kde()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try adding more layers to the FFNN below and experimenting with the dropout rate, hidden layer sizes, and different choices of non-linearity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFNN_Experiment(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, output_dim, hidden_dim=100):\n",
    "        super().__init__()\n",
    "\n",
    "        hidden_dim=100\n",
    "        \n",
    "        # the first layer transforms an input of size input_dim (e.g., 1000 above) to an output of size hidden_dim (e.g., 100)\n",
    "        self.linear1 = torch.nn.Linear(input_dim, hidden_dim)\n",
    "\n",
    "        # a dropout layer randomly sets the output from the previous layer to 0 p% of the time\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "\n",
    "        # the second layer transforms an input of size hidden_dim (e.g., 100) to an output of size output_dim (e.g., 2 classes for positive/negative)       \n",
    "        self.linear2 = torch.nn.Linear(hidden_dim, output_dim)\n",
    "    \n",
    "    def forward(self, input): \n",
    "        # pass the input through the first layer\n",
    "        layer1_output = self.linear1(input)\n",
    "        \n",
    "        # pass that output through a non-linearity (here, tanh)\n",
    "        # alternatives include torch.relu and torch.sigmoid\n",
    "        layer1_output = torch.tanh(layer1_output)\n",
    "        \n",
    "        # then dropout some outputs during training time (not test time)\n",
    "        layer1_output=self.dropout(layer1_output)\n",
    "        \n",
    "        # and then pass the output from that first layer as input to the second layer\n",
    "        layer2_output = self.linear2(layer1_output)\n",
    "\n",
    "        return layer2_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ffnn_e=FFNN_Experiment(1000, 2, hidden_dim=100)\n",
    "dev_accuracy=train(ffnn_e, \"ffnn_e.model\", train_batches_x, train_batches_y, dev_batches_x, dev_batches_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
