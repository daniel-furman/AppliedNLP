{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thie notebook explores using CNN for binary text classification using the pytorch library. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import nltk\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(x, y, batch_size=12):\n",
    "    batches_x=[]\n",
    "    batches_y=[]\n",
    "    for i in range(0, len(x), batch_size):\n",
    "        xbatch=x[i:i+batch_size]\n",
    "        ybatch=y[i:i+batch_size]\n",
    "        \n",
    "        maxlen=max([len(sent) for sent in xbatch])\n",
    "        \n",
    "        # pad sequence with 0's to maximum sequence length within that batch\n",
    "        for j in range(len(xbatch)):\n",
    "            xbatch[j].extend([0] * (maxlen-len(xbatch[j])))\n",
    "                        \n",
    "        batches_x.append(torch.LongTensor(xbatch))\n",
    "        batches_y.append(torch.LongTensor(ybatch))\n",
    "    \n",
    "    return batches_x, batches_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_INDEX = 0             # reserved for padding words\n",
    "UNKNOWN_INDEX = 1         # reserved for unknown words\n",
    "\n",
    "data_lens = []\n",
    "\n",
    "def read_embeddings(filename, vocab_size=100000):\n",
    "    \"\"\"\n",
    "  Utility function, loads in the `vocab_size` most common embeddings from `filename`\n",
    "  \n",
    "  Arguments:\n",
    "  - filename:     path to file\n",
    "                  automatically infers correct embedding dimension from filename\n",
    "  - vocab_size:   maximum number of embeddings to load\n",
    "\n",
    "  Returns \n",
    "  - embeddings:   torch.FloatTensor matrix of size (vocab_size x word_embedding_dim)\n",
    "  - vocab:        dictionary mapping word (str) to index (int) in embedding matrix\n",
    "  \"\"\"\n",
    "\n",
    "  # get the embedding size from the first embedding\n",
    "    with open(filename, encoding=\"utf-8\") as file:\n",
    "        word_embedding_dim = len(file.readline().split(\" \")) - 1\n",
    "\n",
    "    vocab = {}\n",
    "\n",
    "    embeddings = np.zeros((vocab_size, word_embedding_dim))\n",
    "    with open(filename, encoding=\"utf-8\") as file:\n",
    "        for idx, line in enumerate(file):\n",
    "\n",
    "            if idx + 1 >= vocab_size:\n",
    "                break\n",
    "\n",
    "            cols = line.rstrip().split(\" \")\n",
    "            val = np.array(cols[1:])\n",
    "            word = cols[0]\n",
    "            embeddings[idx + 1] = val\n",
    "            vocab[word] = idx + 1\n",
    "  \n",
    "    return torch.FloatTensor(embeddings), vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings, vocab=read_embeddings(\"../data/glove.6B.100d.100K.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_labels(filename):\n",
    "    labels={}\n",
    "    with open(filename) as file:\n",
    "        for line in file:\n",
    "            cols = line.split(\"\\t\")\n",
    "            label = cols[0]\n",
    "            if label not in labels:\n",
    "                labels[label]=len(labels)\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(filename, vocab, labels, max_data_points=1000):\n",
    "    \"\"\"\n",
    "    :param filename: the name of the file\n",
    "    :return: list of tuple ([word index list], label)\n",
    "    as input for the forward and backward function\n",
    "    \"\"\"    \n",
    "    data = []\n",
    "    data_labels = []\n",
    "    with open(filename) as file:\n",
    "        for line in file:\n",
    "            cols = line.split(\"\\t\")\n",
    "            label = cols[0]\n",
    "            text = cols[1]\n",
    "            w_int = []\n",
    "            for w in nltk.word_tokenize(text.lower()):\n",
    "                if w in vocab:\n",
    "                    w_int.append(vocab[w])\n",
    "                else:\n",
    "                    w_int.append(UNKNOWN_INDEX)\n",
    "                    \n",
    "            data.append((w_int))\n",
    "            data_labels.append(labels[label])\n",
    "            \n",
    "\n",
    "    # shuffle the data\n",
    "    tmp = list(zip(data, data_labels))\n",
    "    random.shuffle(tmp)\n",
    "    data, data_labels = zip(*tmp)\n",
    "    \n",
    "    if max_data_points is None:\n",
    "        return data, data_labels\n",
    "    \n",
    "    return data[:max_data_points], data_labels[:max_data_points]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change this to the directory with your data (from the CheckData_TODO.ipynb exercise).  \n",
    "# The directory should contain train.tsv, dev.tsv and test.tsv\n",
    "directory=\"../data/lmrd\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels=read_labels(\"%s/train.tsv\" % directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll limit the training and dev data to 10,000 data points for this exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX, trainY=read_data(\"%s/train.tsv\" % directory, vocab, labels, max_data_points=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "devX, devY=read_data(\"%s/dev.tsv\" % directory, vocab, labels, max_data_points=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testX, testY=read_data(\"%s/test.tsv\" % directory, vocab, labels, max_data_points=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_trainX, batch_trainY=get_batches(trainX, trainY)\n",
    "batch_devX, batch_devY=get_batches(devX, devY)\n",
    "batch_testX, batch_testY=get_batches(testX, testY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNClassifier_bigram(nn.Module):\n",
    "\n",
    "    \"\"\"\n",
    "    CNN with a window size of 2 (i.e., 2grams) and 96 filters\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, pretrained_embeddings):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.num_filters=96\n",
    "        \n",
    "        self.num_labels = 2\n",
    "\n",
    "        _, embedding_dim=pretrained_embeddings.shape\n",
    "        \n",
    "        self.embeddings = nn.Embedding.from_pretrained(pretrained_embeddings, freeze=True)\n",
    "\n",
    "        # convolution over 2 words    \n",
    "        self.conv_2 = nn.Conv1d(embedding_dim, self.num_filters, 2, 1)\n",
    "        \n",
    "        self.fc = nn.Linear(self.num_filters, self.num_labels)\n",
    "\n",
    "    \n",
    "    def forward(self, input): \n",
    "        \n",
    "        # batch_size x max_seq_length x embeddings_size\n",
    "        x0 = self.embeddings(input)\n",
    "        \n",
    "        # batch_size x embeddings_size x max_seq_length\n",
    "        # (the input order expected by nn.Conv1d)\n",
    "        x0 = x0.permute(0, 2, 1)\n",
    "\n",
    "        # convolution\n",
    "        x2 = self.conv_2(x0)\n",
    "        # non-linearity\n",
    "        x2 = torch.tanh(x2)\n",
    "        # global max-pooling over the entire sequence\n",
    "        x2=torch.max(x2, 2)[0]\n",
    "\n",
    "        out = self.fc(x2)\n",
    "        \n",
    "        return out        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNClassifier_unigram_bigram(nn.Module):\n",
    "\n",
    "    \"\"\"\n",
    "    CNN over window sizes of 1 (unigrams) and 2 (bigrams) each 96 filters, where a document\n",
    "    is representated as the concatentation of the 96 ungram filters + 96 bigram filters.\n",
    "    \n",
    "    \"\"\"\n",
    "        \n",
    "    def __init__(self, pretrained_embeddings):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.num_filters=96\n",
    "        \n",
    "        self.num_labels = 2\n",
    "\n",
    "        _, embedding_dim=pretrained_embeddings.shape\n",
    "        \n",
    "        self.embeddings = nn.Embedding.from_pretrained(pretrained_embeddings, freeze=True)\n",
    "\n",
    "        # convolution over 1 word\n",
    "        self.conv_1 = nn.Conv1d(embedding_dim, self.num_filters, 1, 1)\n",
    "\n",
    "        # convolution over 2 words    \n",
    "        self.conv_2 = nn.Conv1d(embedding_dim, self.num_filters, 2, 1)\n",
    "\n",
    "        self.fc = nn.Linear(self.num_filters*2, self.num_labels)\n",
    "\n",
    "    \n",
    "    def forward(self, input): \n",
    "        \n",
    "        # batch_size x max_seq_length x embeddings_size\n",
    "        x0 = self.embeddings(input)\n",
    "        \n",
    "        # batch_size x embeddings_size x max_seq_length\n",
    "        # (the input order expected by nn.Conv1d)\n",
    "        x0 = x0.permute(0, 2, 1)\n",
    "\n",
    "        # convolution\n",
    "        x1 = self.conv_1(x0)\n",
    "        # non-linearity\n",
    "        x1 = torch.tanh(x1)\n",
    "        # global max-pooling over the entire sequence\n",
    "        x1=torch.max(x1, 2)[0]\n",
    "\n",
    "        x2 = self.conv_2(x0)\n",
    "        x2 = torch.tanh(x2)\n",
    "        x2=torch.max(x2, 2)[0]\n",
    "\n",
    "        combined=torch.cat([x1, x2], dim=1)\n",
    "\n",
    "        out = self.fc(combined)\n",
    "        \n",
    "        return out        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, x, y):\n",
    "    model.eval()\n",
    "    corr = 0.\n",
    "    total = 0.\n",
    "    with torch.no_grad():\n",
    "        for x, y in zip(x, y):\n",
    "            y_preds=model.forward(x)\n",
    "            for idx, y_pred in enumerate(y_preds):\n",
    "                prediction=torch.argmax(y_pred)\n",
    "                if prediction == y[idx]:\n",
    "                    corr += 1.\n",
    "                total+=1                          \n",
    "    return corr/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, x):\n",
    "    model.eval()\n",
    "    preds=[]\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_x in x:\n",
    "            y_preds=model.forward(batch_x).numpy()\n",
    "            for y_pred in y_preds:\n",
    "                prediction=np.argmax(y_pred)\n",
    "                preds.append(prediction)\n",
    "                \n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, model_filename, train_batches_x, train_batches_y, dev_batches_x, dev_batches_y):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "    losses = []\n",
    "    cross_entropy=nn.CrossEntropyLoss()\n",
    "\n",
    "    best_dev_acc=0.\n",
    "    \n",
    "    for epoch in range(5):\n",
    "        model.train()\n",
    "\n",
    "        for x, y in zip(train_batches_x, train_batches_y):\n",
    "            y_pred=model.forward(x)\n",
    "            loss = cross_entropy(y_pred.view(-1, 2), y.view(-1))\n",
    "            losses.append(loss)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        dev_accuracy=evaluate(model, dev_batches_x, dev_batches_y)\n",
    "        \n",
    "        # we're going to save the model that performs the best on *dev* data\n",
    "        if dev_accuracy > best_dev_acc:\n",
    "            torch.save(model.state_dict(), model_filename)\n",
    "            print(\"%.3f is better than %.3f, saving model ...\" % (dev_accuracy, best_dev_acc))\n",
    "            best_dev_acc = dev_accuracy\n",
    "        if epoch % 1 == 0:\n",
    "            print(\"Epoch %s, dev accuracy: %.3f\" % (epoch, dev_accuracy))\n",
    "            \n",
    "    model.load_state_dict(torch.load(model_filename))            \n",
    "    print(\"\\nBest Performing Model achieves dev accuracy of : %.3f\" % (best_dev_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's examine the performance of a CNN that only has access to bigram features (from a CNN window size of 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model = CNNClassifier_bigram(pretrained_embeddings=embeddings)\n",
    "train(cnn_model, \"cnn.bigram.model\", batch_trainX, batch_trainY, batch_devX, batch_devY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's add unigram features to the bigram features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model = CNNClassifier_unigram_bigram(pretrained_embeddings=embeddings)\n",
    "train(cnn_model, \"cnn.unigram_bigram.model\", batch_trainX, batch_trainY, batch_devX, batch_devY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1: Experiment with the network structure that works best for your binary classification dataset.  Explore the following choices: a.) the order of ngrams (window size);  b.) the number of filters; c.) the activation functions; d.) the use of dropout.  Which architecture performs best on the **development data**?  (Remember, never optimize this choice on your test data!) Create 5 different models and execute them below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_model1(nn.Module):\n",
    "\n",
    "    \"\"\"\n",
    "    Use this bigram CNN as a starting point for developing your own custom model.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, pretrained_embeddings):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.num_filters=96\n",
    "        \n",
    "        self.num_labels = 2\n",
    "\n",
    "        _, embedding_dim=pretrained_embeddings.shape\n",
    "        \n",
    "        self.embeddings = nn.Embedding.from_pretrained(pretrained_embeddings, freeze=True)\n",
    "\n",
    "        # convolution over 2 words    \n",
    "        self.conv_2 = nn.Conv1d(embedding_dim, self.num_filters, 2, 1)\n",
    "        \n",
    "        self.fc = nn.Linear(self.num_filters, self.num_labels)\n",
    "\n",
    "    \n",
    "    def forward(self, input): \n",
    "        \n",
    "        # batch_size x max_seq_length x embeddings_size\n",
    "        x0 = self.embeddings(input)\n",
    "        \n",
    "        # batch_size x embeddings_size x max_seq_length\n",
    "        # (the input order expected by nn.Conv1d)\n",
    "        x0 = x0.permute(0, 2, 1)\n",
    "\n",
    "        x2 = self.conv_2(x0)\n",
    "        x2 = torch.tanh(x2)\n",
    "        x2=torch.max(x2, 2)[0]\n",
    "\n",
    "        out = self.fc(x2)\n",
    "        \n",
    "        return out        \n",
    "\n",
    "cnn_model1 = CNN_model1(pretrained_embeddings=embeddings)\n",
    "train(cnn_model1, \"cnn.1.model\", batch_trainX, batch_trainY, batch_devX, batch_devY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_model2(nn.Module):\n",
    "\n",
    "    \"\"\"\n",
    "    Use this bigram CNN as a starting point for developing your own custom model.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, pretrained_embeddings):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.num_filters=96\n",
    "        \n",
    "        self.num_labels = 2\n",
    "\n",
    "        _, embedding_dim=pretrained_embeddings.shape\n",
    "        \n",
    "        self.embeddings = nn.Embedding.from_pretrained(pretrained_embeddings, freeze=True)\n",
    "\n",
    "        # convolution over 2 words    \n",
    "        self.conv_2 = nn.Conv1d(embedding_dim, self.num_filters, 2, 1)\n",
    "        \n",
    "        self.fc = nn.Linear(self.num_filters, self.num_labels)\n",
    "\n",
    "    \n",
    "    def forward(self, input): \n",
    "        \n",
    "        # batch_size x max_seq_length x embeddings_size\n",
    "        x0 = self.embeddings(input)\n",
    "        \n",
    "        # batch_size x embeddings_size x max_seq_length\n",
    "        # (the input order expected by nn.Conv1d)\n",
    "        x0 = x0.permute(0, 2, 1)\n",
    "\n",
    "        x2 = self.conv_2(x0)\n",
    "        x2 = torch.tanh(x2)\n",
    "        x2=torch.max(x2, 2)[0]\n",
    "\n",
    "        out = self.fc(x2)\n",
    "        \n",
    "        return out        \n",
    "\n",
    "cnn_model2 = CNN_model2(pretrained_embeddings=embeddings)\n",
    "train(cnn_model2, \"cnn.2.model\", batch_trainX, batch_trainY, batch_devX, batch_devY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_model3(nn.Module):\n",
    "\n",
    "    \"\"\"\n",
    "    Use this bigram CNN as a starting point for developing your own custom model.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, pretrained_embeddings):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.num_filters=96\n",
    "        \n",
    "        self.num_labels = 2\n",
    "\n",
    "        _, embedding_dim=pretrained_embeddings.shape\n",
    "        \n",
    "        self.embeddings = nn.Embedding.from_pretrained(pretrained_embeddings, freeze=True)\n",
    "\n",
    "        # convolution over 2 words    \n",
    "        self.conv_2 = nn.Conv1d(embedding_dim, self.num_filters, 2, 1)\n",
    "        \n",
    "        self.fc = nn.Linear(self.num_filters, self.num_labels)\n",
    "\n",
    "    \n",
    "    def forward(self, input): \n",
    "        \n",
    "        # batch_size x max_seq_length x embeddings_size\n",
    "        x0 = self.embeddings(input)\n",
    "        \n",
    "        # batch_size x embeddings_size x max_seq_length\n",
    "        # (the input order expected by nn.Conv1d)\n",
    "        x0 = x0.permute(0, 2, 1)\n",
    "\n",
    "        x2 = self.conv_2(x0)\n",
    "        x2 = torch.tanh(x2)\n",
    "        x2=torch.max(x2, 2)[0]\n",
    "\n",
    "        out = self.fc(x2)\n",
    "        \n",
    "        return out        \n",
    "\n",
    "cnn_model3 = CNN_model3(pretrained_embeddings=embeddings)\n",
    "train(cnn_model3, \"cnn.3.model\", batch_trainX, batch_trainY, batch_devX, batch_devY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_model4(nn.Module):\n",
    "\n",
    "    \"\"\"\n",
    "    Use this bigram CNN as a starting point for developing your own custom model.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, pretrained_embeddings):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.num_filters=96\n",
    "        \n",
    "        self.num_labels=2\n",
    "        \n",
    "        _, embedding_dim=pretrained_embeddings.shape\n",
    "        \n",
    "        self.embeddings = nn.Embedding.from_pretrained(pretrained_embeddings, freeze=True)\n",
    "\n",
    "        # convolution over 2 words    \n",
    "        self.conv_2 = nn.Conv1d(embedding_dim, self.num_filters, 2, 1)\n",
    "        \n",
    "        self.fc = nn.Linear(self.num_filters, self.num_labels)\n",
    "\n",
    "    \n",
    "    def forward(self, input): \n",
    "        \n",
    "        # batch_size x max_seq_length x embeddings_size\n",
    "        x0 = self.embeddings(input)\n",
    "        \n",
    "        # batch_size x embeddings_size x max_seq_length\n",
    "        # (the input order expected by nn.Conv1d)\n",
    "        x0 = x0.permute(0, 2, 1)\n",
    "\n",
    "        x2 = self.conv_2(x0)\n",
    "        x2 = torch.tanh(x2)\n",
    "        x2=torch.max(x2, 2)[0]\n",
    "\n",
    "        out = self.fc(x2)\n",
    "        \n",
    "        return out        \n",
    "\n",
    "cnn_model4 = CNN_model4(pretrained_embeddings=embeddings)\n",
    "train(cnn_model4, \"cnn.4.model\", batch_trainX, batch_trainY, batch_devX, batch_devY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_model5(nn.Module):\n",
    "\n",
    "    \"\"\"\n",
    "    Use this bigram CNN as a starting point for developing your own custom model.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, pretrained_embeddings):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.num_filters=96\n",
    "        \n",
    "        self.num_labels = 2\n",
    "\n",
    "        _, embedding_dim=pretrained_embeddings.shape\n",
    "        \n",
    "        self.embeddings = nn.Embedding.from_pretrained(pretrained_embeddings, freeze=True)\n",
    "\n",
    "        # convolution over 2 words    \n",
    "        self.conv_2 = nn.Conv1d(embedding_dim, self.num_filters, 2, 1)\n",
    "        \n",
    "        self.fc = nn.Linear(self.num_filters, self.num_labels)\n",
    "\n",
    "    \n",
    "    def forward(self, input): \n",
    "        \n",
    "        # batch_size x max_seq_length x embeddings_size\n",
    "        x0 = self.embeddings(input)\n",
    "        \n",
    "        # batch_size x embeddings_size x max_seq_length\n",
    "        # (the input order expected by nn.Conv1d)\n",
    "        x0 = x0.permute(0, 2, 1)\n",
    "\n",
    "        x2 = self.conv_2(x0)\n",
    "        x2 = torch.tanh(x2)\n",
    "        x2=torch.max(x2, 2)[0]\n",
    "\n",
    "        out = self.fc(x2)\n",
    "        \n",
    "        return out        \n",
    "\n",
    "cnn_model5 = CNN_model5(pretrained_embeddings=embeddings)\n",
    "train(cnn_model5, \"cnn.5.model\", batch_trainX, batch_trainY, batch_devX, batch_devY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can generate predictions for a given test set with the `predict` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gold data for test\n",
    "gold=[]\n",
    "for batchY in batch_testY:\n",
    "    gold.extend(batchY)\n",
    "\n",
    "# prediction data for test\n",
    "model=cnn_model5\n",
    "predictions=predict(model, batch_testX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2: For the single model that performed best on the dev data (that you identified in Q1 above), calculate its 95% confidence intervals for accuracy on the **test data**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
