{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In `acl.all.tsv` you'll find 7,188 papers published at major NLP venues (ACL, EMNLP, NAACL, TACL, etc.) between 2013 and 2020.  Your job is to use topic modeling to discover what topics in NLP have been increasing or decreasing in use over this time.  Here is a sample of the data we'll use:\n",
    "\n",
    "|id|year of publication|title|abstract|\n",
    "|---|---|---|---|\n",
    "|pimentel-etal-2020-phonotactic|2020|Phonotactic Complexity and Its Trade-offs|We present methods for calculating a measure of phonotactic complexity---bits per phoneme--- that permits a straightforward cross-linguistic comparison. When given a word, represented as a sequence of phonemic segments such as symbols in the international phonetic alphabet, and a statistical model trained on a sample of word types from the language, we can approximately measure bits per phoneme using the negative log-probability of that word under the model. This simple measure allows us to compare the entropy across languages, giving insight into how complex a language's phonotactics is. Using a collection of 1016 basic concept words across 106 languages, we demonstrate a very strong negative correlation of âˆ’ 0.74 between bits per phoneme and the average length of words.|\n",
    "|wang-etal-2020-amr|2020|AMR-To-Text Generation with Graph Transformer|Abstract meaning representation (AMR)-to-text generation is the challenging task of generating natural language texts from AMR graphs, where nodes represent concepts and edges denote relations. The current state-of-the-art methods use graph-to-sequence models; however, they still cannot significantly outperform the previous sequence-to-sequence models or statistical approaches. In this paper, we propose a novel graph-to-sequence model (Graph Transformer) to address this task. The model directly encodes the AMR graphs and learns the node representations. A pairwise interaction function is used for computing the semantic relations between the concepts. Moreover, attention mechanisms are used for aggregating the information from the incoming and outgoing neighbors, which help the model to capture the semantic information effectively. Our model outperforms the state-of-the-art neural approach by 1.5 BLEU points on LDC2015E86 and 4.8 BLEU points on LDC2017T10 and achieves new state-of-the-art performances.|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's read in the data and train a topic model on it (you are free to set the number of topics $K$ as you see fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K=50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "import operator\n",
    "\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter(word, stopwords):\n",
    "    \n",
    "    \"\"\" Function to exclude words from a text \"\"\"\n",
    "    \n",
    "    # no stopwords\n",
    "    if word in stopwords:\n",
    "        return False\n",
    "    \n",
    "    # has to contain at least one letter\n",
    "    if re.search(\"[A-Za-z]\", word) is not None:\n",
    "        return True\n",
    "    \n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_docs(dataFile, stopwords):\n",
    "    \n",
    "    names=[]    \n",
    "    docs=[]\n",
    "   \n",
    "    with open(dataFile, encoding=\"utf-8\") as file:\n",
    "        for line in file:\n",
    "            cols=line.rstrip().split(\"\\t\")\n",
    "            idd=cols[0]\n",
    "            name=cols[2]\n",
    "            year=int(cols[1])\n",
    "        \n",
    "            text=cols[3]\n",
    "            \n",
    "            tokens=nltk.word_tokenize(text.lower())\n",
    "            tokens=[x for x in tokens if filter(x, stopwords)]\n",
    "            docs.append(tokens)\n",
    "            names.append((name, year))\n",
    "    return docs, names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataFile=\"../data/acl.all.tsv\"\n",
    "data, doc_names=read_docs(dataFile, stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will convert the data into a bag-of-words representation using gensim's [corpora.dictionary](https://radimrehurek.com/gensim/corpora/dictionary.html) methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create vocab from data; restrict vocab to only the top 10K terms that show up in at least 5 documents \n",
    "# and no more than 50% of all documents\n",
    "\n",
    "dictionary = corpora.Dictionary(data)\n",
    "dictionary.filter_extremes(no_below=5, no_above=.5, keep_n=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace dataset with numeric ids words in vocab (and exclude all other words)\n",
    "corpus = [dictionary.doc2bow(text) for text in data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's run a topic model on this data using gensim's built-in LDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=dictionary,\n",
    "                                           num_topics=K, \n",
    "                                           passes=10,\n",
    "                                           alpha='auto')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get a sense of what the topics are by printing the top 10 words with highest $P(word \\mid topic)$ for each topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(K):\n",
    "    print(\"topic %s:\\t%s\" % (i, ' '.join([term for term, freq in lda_model.show_topic(i, topn=10)])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's print out the documents that have the highest topic representation -- i.e., for a given topic $k$, the documents with highest $P(topic=k | document)$ -- in order to ground these topic summaries with actual documents that contain those topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model=lda_model \n",
    "\n",
    "topic_docs=[]\n",
    "for i in range(K):\n",
    "    topic_docs.append({})\n",
    "for doc_id in range(len(corpus)):\n",
    "    doc_topics=topic_model.get_document_topics(corpus[doc_id])\n",
    "    for topic_num, topic_prob in doc_topics:\n",
    "        topic_docs[topic_num][doc_id]=topic_prob\n",
    "\n",
    "for i in range(K):\n",
    "    print(\"%s\\n\" % ' '.join([term for term, freq in topic_model.show_topic(i, topn=10)]))\n",
    "    sorted_x = sorted(topic_docs[i].items(), key=operator.itemgetter(1), reverse=True)\n",
    "    for k, v in sorted_x[:5]:\n",
    "        print(\"%s\\t%.3f\\t%s\" % (i,v,doc_names[k]))\n",
    "    print()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1**: Use this basic framework to plot the distribution of a topic over time (between the years 2013-2020).  Specifically, given a document-topic distribution $\\theta$ for an entire corpus such that $\\theta_d$ is the document-topic distribution for document $d$ and $\\theta_{d,i}$ is the probability of topic $i$ in document $d$, the relative frequency of topic $i$ at time $t$ is the sum of $\\theta_{d,i}$ for all documents that are published in year $t$, divided by the total number of documents published in year $t$: \n",
    "\n",
    "$$\n",
    "f(t, i) ={ \\sum_{d \\in D: date(d) = t} \\theta_{d,i} \\over \\sum_{d \\in D: date(d) = t} 1}\n",
    "$$\n",
    "\n",
    "For all of the $K$ topics you learn above, plot the distribution of that topic over time (i.e., the x-axis should be time from 2013-2020 and the y-axis should be the relative frequency value $f$ defined above).  Your output here should be $K$ line charts paired with their topic signatures (e.g., from `topic_model.show_topic`, so that we know what topic the chart corresponds to).  See below for sample code generating such a line chart with x, y inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "a=[2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020]\n",
    "b=[0.4, 0.3, 0.1, 0.9, 0.04, 0.5, 0.45, 0.3 ]\n",
    "\n",
    "def plot_category(a,b):\n",
    "    plt.plot(a,b)\n",
    "    plt.show()\n",
    "\n",
    "plot_category(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
